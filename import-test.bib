@inproceedings{Yang2019XLNetGA,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Z. Yang and Zihang Dai and Yiming Yang and J. Carbonell and R. Salakhutdinov and Quoc V. Le},
  booktitle={NeurIPS},
  year={2019}
}

@InProceedings{	  ovadia2019can,
  title		= {Can you trust your model's uncertainty? Evaluating
		  predictive uncertainty under dataset shift},
  author	= {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado,
		  Zachary and Sculley, David and Nowozin, Sebastian and
		  Dillon, Joshua and Lakshminarayanan, Balaji and Snoek,
		  Jasper},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {13991--14002},
  year		= {2019}
}

@article{richter2016gta5,
	title={Playing for Data: Ground Truth from Computer Games},
	ISBN={9783319464756},
	ISSN={1611-3349},
	url={http://dx.doi.org/10.1007/978-3-319-46475-6_7},
	DOI={10.1007/978-3-319-46475-6_7},
	journal={Lecture Notes in Computer Science},
	publisher={Springer International Publishing},
	author={Richter, Stephan R. and Vineet, Vibhav and Roth, Stefan and Koltun, Vladlen},
	year={2016},
	pages={102â€“118}
}

@inproceedings{wang-etal-2017-gated,
    title = "Gated Self-Matching Networks for Reading Comprehension and Question Answering",
    author = "Wang, Wenhui  and
      Yang, Nan  and
      Wei, Furu  and
      Chang, Baobao  and
      Zhou, Ming",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1018",
    doi = "10.18653/v1/P17-1018",
    pages = "189--198",
    abstract = "In this paper, we present the gated self-matching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3{\%} on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9{\%}. At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.",
}
